<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SuperDec">
  <meta name="keywords" content="SuperDec, Superquadrics, Scene Decompostion, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SuperDec</title>
  <link rel="icon" type="image/png" href="./static/figures/compressed/logo/chair.png">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.svg">
  <!-- <link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
  integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T"
  crossorigin="anonymous"
  /> -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="./static/js/video_comparison.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> SuperDec <img src="static/figures/compressed/logo/chair.png" width="100" style="position: relative; top: 15px; left: -30px;"> </h1> 
          <h2 class="title is-2 publication-title">3D Scene Decomposition with <br>Superquadric Primitives</h2>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://elisabettafedele.github.io/">Elisabetta Fedele</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://boysun045.github.io/boysun-website/">Boyang Sun</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://geometry.stanford.edu/?member=guibas">Leonidas Guibas</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://people.inf.ethz.ch/pomarc">Marc Pollefeys</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://francisengelmann.github.io/">Francis Engelmann</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>ETH Zurich</span> &nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>Stanford University</span> &nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>3</sup>Microsoft</span> &nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <span class="link-block">
                <a href="https://arxiv.org/abs/2504.00992"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark"
                   style="background-color: #5c5c5c">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-light is-small has-text-centered">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Decomposition of Replica scenes</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item room0_1_bg">
          <img src="./static/figures/compressed/teaser/room0_1_bg.jpeg">
        </div>
        <div class="item room1_2_bg">
          <img src="./static/figures/compressed/teaser/room1_2_bg.jpeg">
        </div>
        <div class="item room2_1_bg">
          <img src="./static/figures/compressed/teaser/room2_1_bg.jpeg">
        </div>
        <div class="item office1_1_bg">
          <img src="./static/figures/compressed/teaser/office1_1_bg.jpeg">
        </div>
        <div class="item office4_1_bg">
          <img src="./static/figures/compressed/teaser/office4_3_bg.jpeg">
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          We present <span class="dnerf">SuperDec</span>, an approach for compact 3D scene representations based on geometric primitives, namely superquadrics.
          While most recent works leverage geometric primitives to obtain photorealistic 3D scene representations, we propose to leverage them to obtain a compact yet expressive representation. 
          We propose to solve the problem locally on individual objects and leverage the capabilities of instance segmentation methods to scale our solution to full 3D scenes. 
          In doing that, we design a new architecture which efficiently decompose point clouds of arbitrary objects in a compact set of superquadrics. 
          We train our architecture on ShapeNet and we prove its generalization capabilities on object instances extracted from the ScanNet++ dataset as well as on full Replica scenes. 
          Finally, we show how a compact representation based on superquadrics can be useful for a diverse range of downstream applications, including robotic tasks and controllable visual content generation and editing.
        </div>
      </div>
    </div>

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method Overview</h2>
        <div class="content has-text-justified">
          <img src="./static/figures/compressed/method/method.jpeg">
          <p><strong>Overview of SuperDec</strong>. Given a point cloud of an object with N points, a Transformer-based <i>neural network</i> predicts parameters for P superquadrics, as well as a soft segmentation matrix that assigns points to superquadrics. 
            The predicted parameters include the 11 superquadric parameters and an objectness score. 
            These predictions provide an effective initialization for the subsequent Levenberg–Marquardt (LM) <i>optimization</i>, which refines the superquadrics.</p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section" style="width: 85%; margin: 0 auto">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2> 
        <h2
          class="title is-4"
          style="text-align: justify; padding-bottom: 10px"
        >
        Object-Level Results
        </h2>
        <div class="content has-text-justified">
          <p> We evaluate the performance of <span class="dnerf">SuperDec</span> to decompose individual objects on <em>ShapeNet</em>, a traditional object dataset.</p>
        <p>To evaluate both accuracy and generalization, we conduct two experiments: <em>in-category</em> and <em>out-of-category</em>. 
          In the <em>in-category</em> experiment, all learning-based methods are trained on the full ShapeNet training set (13 classes) and evaluated on the corresponding test set.
                              In the <em>out-of-category</em> experiment, models are trained on half of the categories (airplane, bench, chair, lamp, rifle, table) and evaluated on the remaining ones (car, sofa, loudspeaker, cabinet, display, telephone, watercraft).
                              </p>
                              <p>
                                We compare with three baselines: <em>EMS</em> [2],  <em>SQ</em> [3], <em>CSA</em> [4].
                              </p>
      </div>
    </div>
  </div>
</div>
      <div class="container">
        <div class="columns is-centered ">
          <div class="column is-full-width">
        <div
          class="slideshow-container"
          style="width: 70%; margin-top: 30px"
        >
     

          <div class="mySlides0">
            <div class="card">
              <div style="display: flex; align-items: center">
                <div style="flex: 1; text-align: center;">
                  <!-- <p><strong><font size="5">Quantitative Comparison</font></strong></p><br> -->
                  <figure style="display: inline-block;">
                    <img
                      src="./static/figures/compressed/shapenet/quanti.jpeg"
                      alt="shapenet_quantitative"
                      width="900"
                    />
                    <figcaption style="margin-top: 0.5rem;">
                      <strong>Quantitative Results on ShapeNet</strong>. We evaluate the accuracy of the reconstruction in terms of L2 Chamfer distance (scaled by 100) and the compactness of the representation in terms of number of primitives.


                    </figcaption>
                  </figure>
                </div>
              </div>
            </div>
          </div>
          <div class="mySlides0">
            <div class="card">
              <div style="display: flex; align-items: center">
                <div style="flex: 1">
                  <!-- <p><strong><font size="5">  Qualitative Comparison</font></strong> <br><br> -->
                    <!-- Our method greatly outperforms both learned and non-learned baselines. Compared to <i>learned</i> baselines, we achieve an L2 loss <b>6x</b> smaller while using nearly half the number of primitives. 
                    This verifies our hypothesis that leveraging local point features benefits 3D decomposition in terms of both accuracy and compactness. Against the <i>non-learned</i> baseline, we predict a similar number of primitives but achieve an L2 loss <b>19x</b> smaller. 
                    This verifies our hypothesis that learning shape priors allows to avoid local minima which are sometimes unavoidable using pure optimization.</p>
                    <br> -->
                    <figure style="display: inline-block;">
                  <img
                    src="./static/figures/compressed/shapenet/quali.jpeg"
                    alt="shapenet_qualitative"
                    width="750"
                  />
                  <figcaption style="margin-top: 0.5rem;">
                    <strong>Qualitative Results on ShapeNet</strong>. We show results on test samples for in-category <em>(four first columns)</em> classes and out-of-category classes <em>(two last columns)</em>. The latter were not seen during training and illustrate how well models generalize to novel classes.

                  </figcaption>
                </figure>
                </div>
              </div>
            </div>
          </div>
          

          <!-- Next and previous buttons -->
          <a class="prev" onclick="plusSlides(0, -1)">&#10094;</a>
          <a class="next" onclick="plusSlides(0, 1)">&#10095;</a>
        </div>
        <br />

        <!-- The dots/circles -->
        <div style="text-align: center">
          <span class="dot0" onclick="currentSlide(0, 1)"></span>
          <span class="dot0" onclick="currentSlide(0, 2)"></span>
        </div>
      </div>
    </div>
  </div>

    <div class="container is-max-desktop">
      <h2
          class="title is-4"
          style="text-align: justify; padding-bottom: 10px"
        >
        Scene-Level Results
        </h2>
      <div class="content has-text-justified">
      <p>Our model trained only on 13 ShapeNet categories, can be extended to 3D scenes without any additional fine-tuning. Specifically, 
          given a 3D scene point cloud, we obtain object instance mask using Mask3D, center and rescale them, and directly input them into our model. We visualize the results for some scenes from the Replica dataset.</p>
        <br></div>
      <figure style="text-align: center; margin-top: -15px; margin-bottom: -10px;">
        <div style="display: flex; justify-content: center; gap: 0.3rem;">
        
        <div style="text-align: center;">
          <div style="margin-bottom: 0.5rem;"><strong>Room 0</strong></div>
          <img src="./static/figures/compressed/replica/room0_rgb.jpeg" style="width: 230px;">
        </div>
        <div style="text-align: center;">
          <div style="margin-bottom: 0.5rem;"><strong>Room 1</strong></div>
          <img src="./static/figures/compressed/replica/room1_rgb.jpeg" style="width: 230px;">
        </div>
        <div style="text-align: center;">
          <div style="margin-bottom: 0.5rem;"><strong>Room 2</strong></div>
          <img src="./static/figures/compressed/replica/room2_rgb.jpeg" style="width: 230px;">
        </div>
        <div style="text-align: center;">
          <div style="margin-bottom: 0.5rem;"><strong>Office 4</strong></div>
          <img src="./static/figures/compressed/replica/office_rgb.jpeg" style="width: 230px;">
        </div>
      </div>
      <div style="display: flex; justify-content: center; gap: 0.3rem;">
        <div style="text-align: center;">
          <img src="./static/figures/compressed/replica/room0_sq.jpeg" style="width: 230px;">
        </div>
        <div style="text-align: center;">
          <img src="./static/figures/compressed/replica/room1_sq.jpeg" style="width: 230px;">
        </div>
        <div style="text-align: center;">
          <img src="./static/figures/compressed/replica/room2_sq.jpeg" style="width: 230px;">
        </div>
        <div style="text-align: center;">
          <img src="./static/figures/compressed/replica/office_sq.jpeg" style="width: 230px;">
        </div>
      </div>
      <figcaption style="margin-top: 10px;">
        <strong>Qualitative results on Replica scenes.</strong>
        Top row shows renderings of the original point clouds, bottom rows shows the superquadric representation obtained with <span class="dnerf">SuperDec</span>.
      </figcaption>
    </figure>

  </div>
</section>






<section class="section" style="width: 85%; margin: 0 auto">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2
          class="title is-3"
          style="text-align: center; padding-bottom: 10px"
        >
        Applications
        </h2>
        <div class="content has-text-justified">
        <p>We envision <span class="dnerf">SuperDec</span> enabling a wide range of applications, especially in <b>robotics</b> and <b>controllable content</b> generation.</p>
      <h2
          class="title is-4"
          style="text-align: justify; padding-bottom: 10px"
        >
        Robotics
        </h2>
        <div class="content has-text-justified">
          <p>We have explored how our representation can be used in robotics by evaluating in real-world for the tasks of <i>path planning</i> and <i>object grasping</i>. 
            Given a scan of a real-world 3D scene captured with an iPad, we use <span class="dnerf">SuperDec</span> to compute its superquadric representation and we compute the grasping poses for some of the objects present in the scene.  
          </div>  
  
          <div class="columns is-vcentered is-centered" style="gap: 0.5rem;">
            <div class="column is-narrow is-flex is-flex-direction-column is-align-items-center is-justify-content-center is-fullheight">
              <figure class="image" style="text-align: center;">
              
                <img src="./static/figures/compressed/robotics/grasp_bottle_flowers.jpeg" alt="Image 1" style="height: 220px; width: auto;">
                <figcaption style="font-style: italic; margin-bottom: 0.3rem;">Grasp for a <span style="color: rgb(205, 6, 205);">milk bottle</span> and some 
                  <span style="color: #1791be;">flowers</span></figcaption>
              </figure>
            </div>
            <div class="column is-narrow is-flex is-flex-direction-column is-align-items-center is-justify-content-center is-fullheight">
              <figure class="image" style="text-align: center;">
                
                <img src="./static/figures/compressed/robotics/grasp_plant_table.jpeg" alt="Image 2" style="height: 220px; width: auto;">
                <figcaption style="font-style: italic; margin-bottom: 0.3rem;">Grasp for a <span style="color: #e6b925;">side table</span>, and a 
                  <span style="color: #f77315;">plant</span> </figcaption>
              </figure>
            </div>
          </div>
          
        <div class="content has-text-justified">
            Then, given the robot’s starting position, we use the superquadric representation to compute the path planning towards the milk bottle, allowing it to move towards the desired object and to grasp it using the previously computed pose.

        </div>
        <div class="columns is-1 is-multiline is-mobile"> 

    
          <!-- Video Column -->
          <div class="column is-flex is-justify-content-center">
            <video id="robots" autoplay muted loop playsinline controls style="width: 700px; height: auto;">
              <source src="./static/videos/real-world-exp-1080.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        

       
    </div> 
    <h2
    class="title is-4"
    style="text-align: justify; padding-bottom: 10px"
  >

  <br>
  <br>
  Controllable generation and editing 
  </h2>
  <div class="content has-text-justified">
    <p>We have also explored how our representation can be directly leveraged to introduce joint spatial and semantic control in the generations of text-to-image diffusion models. To do that we generated some images by conditioning a ControlNet on the depths of the superquadrics extracted from some Replica scenes. We have seen that the superquadric representation can be used to achieve both spatial and semantic control of the generations.</p>
      

    </div>
    <figure style="text-align: center; margin-top: -15px; margin-bottom: -10px;">
      <div style="display: flex; justify-content: center; gap: 0.3rem;">
      <div style="text-align: center;">
        <div style="font-style: italic; margin-bottom: 0.5rem;">Original</div>
        <img src="./static/figures/compressed/generation/room1_2_bg.jpeg" style="width: 230px;">
      </div>
      <div style="text-align: center;">
        <div style="font-style: italic; margin-bottom: 0.5rem;">Editing</div>
        <img src="./static/figures/compressed/generation/room1_edit.jpeg" style="width: 230px;">
      </div>
      <div style="text-align: center;">
        <div style="font-style: italic; margin-bottom: 0.5rem;">Addition</div>
        <img src="./static/figures/compressed/generation/room1_double.jpeg" style="width: 230px;">
      </div>
      <div style="text-align: center;">
        <div style="font-style: italic; margin-bottom: 0.5rem;">Deletion</div>
        <img src="./static/figures/compressed/generation/room1_remove.jpeg" style="width: 230px;">
      </div>
    </div>
    <div style="display: flex; justify-content: center; gap: 0.3rem;">
      <div style="text-align: center;">
        <img src="./static/figures/compressed/generation/corner-plant-orig.jpeg" style="width: 230px;">
      </div>
      <div style="text-align: center;">
        <img src="./static/figures/compressed/generation/corner-plant-edit.jpeg" style="width: 230px;">
      </div>
      <div style="text-align: center;">
        <img src="./static/figures/compressed/generation/corner-plant-double.jpeg" style="width: 230px;">
      </div>
      <div style="text-align: center;">
        <img src="./static/figures/compressed/generation/corner-plant-remove.jpeg" style="width: 230px;">
      </div>
    </div>
    <figcaption style="margin-top: 10px;">
      <strong><em>Spatial</em> control.</strong>
      Top row shows superquadrics generated by <span class="dnerf">SuperDec</span>, bottom row shows generated images using the prompt
      <em>"A corner of a room with a plant"</em>.
    </figcaption>
  </figure>
    
    <br>

    <div class="content has-text-justified">
      </div>
      <figure style="text-align: center; margin-top: -15px; margin-bottom: -10px;">

        <!-- Single row with all 4 images -->
        <div style="display: flex; justify-content: center; gap: 0.3rem; flex-wrap:wrap;">
          <div style="text-align: center;">
            <div style="font-style: italic; margin-bottom: 0.5rem;">Superquadrics</div>
            <img src="./static/figures/compressed/generation/room0_1.jpeg" style="width: 230px;">
          </div>
          <div style="text-align: center;">
            <div style="font-style: italic; margin-bottom: 0.5rem;">Depth</div>
            <img src="./static/figures/compressed/generation/room0_depth.jpeg" style="width: 230px;">
          </div>
          <div style="text-align: center;">
            <div style="font-style: italic; margin-bottom: 0.5rem;">“<strong>Pink</strong> living room”</div>
            <img src="./static/figures/compressed/generation/room0_pink.jpeg" style="width: 230px;">
          </div>
          <div style="text-align: center;">
            <div style="font-style: italic; margin-bottom: 0.5rem;"><strong>Modern</strong> living room”</div>
            <img src="./static/figures/compressed/generation/room0_living_room.jpeg" style="width: 230px;">
          </div>
        </div>
      
        <!-- Global caption -->
        <figcaption style="margin-top: 10px;">
          <strong><em>Semantic</em> control.</strong>
          All images share the same scene geometry; the first two are structural prompts, the last two are generated with distinct textual prompts. Our representation allows to change the style of the room while keeping its semantic and geometric structure fixed.
        </figcaption>
      </figure>
      
      

    
  </div>

</section>

<section class="section" style="width: 85%; margin: 0 auto">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2
          class="title is-3"
          style="text-align: center; padding-bottom: 10px"
        >
        Additional experiments
        </h2>
        <h2
          class="title is-4"
          style="text-align: justify; padding-bottom: 10px">
          
          Unsupervised part segmentation
        </h2>
        <div class="content has-text-justified">
          <p>
            Our method not only learns to predict the parameters of the superquadrics representation, but also the segmentation matrix which decomposes the initial point cloud into parts which can be fitted by the predicted superquadrics. Below, we visualize the predicted segmentations for the same examples from ShapeNet. We observe that segmentation masks, appear very sharp and this suggests that our method, especially if trained at a larger scale, can be leveraged for different applications as geometry-based part segmentation or as pretraining for supervised part segmentation. 
          </p>
          </div>  
          <figure style="text-align: center;">
            <!-- Image row -->
            <div style="display: flex;justify-content: center; gap: 0.3rem;">
              <div style="text-align: center;">
              <img src="./static/figures/compressed/additional/d217e8ab61670bbb433009863c91a425_superdec_segment.jpeg" style="width: 230px;">
            </div>
            <div style="text-align: center;">
              <img src="./static/figures/compressed/additional/cbcc5cd1aec9f3413aa677469bbdd68c_superdec_segment.jpeg" style="width: 230px;">
            </div>
            <div style="text-align: center;">
            <img src="./static/figures/compressed/additional/cda92b4188af3a17c03a563a9407c5ea_superdec_segment.jpeg" style="width: 230px; ">
          </div>
          <div style="text-align: center;">
              <img src="./static/figures/compressed/additional/d911b390503a11f96436916a86a90ed7_superdec_segment.jpeg" style="width: 230px; ">
            </div>
          </div>
          
            <!-- Global caption -->
            <figcaption style="margin-top: 0.75rem;">
              <strong>Part segmentation results on ShapeNet</strong>. Our method learns to segment objects into parts which can be fitted by superquadrics. 
            </figcaption>
          </figure>

          <br><br>

          <h2
          class="title is-4"
          style="text-align: justify; padding-bottom: 10px">
          
          What does our network learn?
        </h2>
        <div class="content has-text-justified">
          <p>
            Since our network is completely unsupervised in segmenting objects into parts, in this experiment we analyze the features learned by our Transformer decoder across different object classes.
            To do that, similarly to BERT's [CLS] token, we append a learnable embedding to the sequence of embedded superquadrics.
            While this additional embedding is never explicitly decoded, it intuitively learns to extract meaningful features during self- and cross-attention layers.
            After training the model with this additional embedding, we decode it at test time and save the resulting vectors across different ShapeNet object categories.
            Below, we show a t-SNE visualization of those.
            We observe that categories with consistent object shapes, such as chairs, airplanes, and cars, form distinct clusters,
            whereas classes with greater shape diversity, such as watercraft, spread across a larger region of the plot.
            This result suggests that our model is able to cluster objects based on their geometrical structure, without the need for any annotation.
          </p>

          </div>  

          <figure id="tsne-embs" style="text-align: center; margin-top: 1.5rem;">
            <img
              src="./static/figures/compressed/additional/tsne_visualization.jpeg"
              alt="t-SNE visualization of primitive embeddings"
              style="width: 60%; object-fit: cover;"
            />
            <figcaption style="margin-top: 0.75rem;">
              <strong>t-SNE Visualization of Primitive Embeddings</strong> across different ShapeNet classes.
            </figcaption>
          </figure>
          
      </div>
    </div>
  </div>
</section>


<section class="section" id="References">
  <div class="container is-max-desktop content">
    <h2 class="title">References</h2>
    <ol>
      <li>
        Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. 
        ShapeNet: An Information-Rich 3D Model Repository.
        In <em>arXiv preprint</em>, 2015.
      </li>
      <li>
        Weixiao Liu, Yuwei Wu, Sipu Ruan, and Gregory S. Chirikjian. Robust and accurate superquadric recovery: a probabilistic approach. In <em>International Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022. 
      </li>
      <li>
        Despoina Paschalidou, Ali Osman Ulusoy, and Andreas Geiger. Superquadrics revisited: Learning 3d shape parsing beyond cuboids. In <em>International Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019.
      </li>
      <li>
        Kaizhi Yang and Xuejin Chen. Unsupervised learning for cuboid shape abstraction via joint segmentation from point clouds. In <em>ACM Transactions On Graphics (TOG)</em>, 2021.
      </li>
      <!-- Add more <li> items for additional references -->
    </ol>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{fedele2025superdec,
      title   = {{SuperDec: 3D Scene Decomposition with Superquadric Primitives}},
      author  = {Fedele, Elisabetta and Sun, Boyang and Guibas, Leonidas and Pollefeys, Marc 
        and Engelmann, Francis},
      journal = {arXiv preprint},
      year    = {2025}
    }
  </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a rel="nerfies" href="https://nerfies.github.io/">Nerfies</a> project page
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script src="http://code.jquery.com/jquery.js"></script>

<!-- Fancybox -->
<script src="fancybox/fancybox.umd.js"></script>
<script>
  Fancybox.bind("[data-fancybox]", {
    infinite: false,
    Thumbs: {
      autoStart: false,
    },
  });
</script>

<!-- Javascript Functions -->
<script type="text/javascript">
  let slideIndex = [1, 1, 1, 1, 1];
  let slideId = 0;
  showSlides(slideId, slideIndex[slideId]);
  showSlides(1, slideIndex[1]);
  showSlides(2, slideIndex[2]);
  showSlides(3, slideIndex[3]);
  showSlides(4, slideIndex[4]);

  // Next/previous controls
  function plusSlides(id, n) {
    showSlides(id, (slideIndex[id] += n));
  }

  // Thumbnail image controls
  function currentSlide(id, n) {
    showSlides(id, (slideIndex[id] = n));
  }

  function showSlides(id, n) {
    let i;
    let slides = document.getElementsByClassName("mySlides" + id);
    let dots = document.getElementsByClassName("dot" + id);
    if (n > slides.length) {
      slideIndex[id] = 1;
    }
    if (n < 1) {
      slideIndex[id] = slides.length;
    }
    for (i = 0; i < slides.length; i++) {
      slides[i].style.display = "none";
    }
    for (i = 0; i < dots.length; i++) {
      dots[i].className = dots[i].className.replace(" active", "");
    }
    slides[slideIndex[id] - 1].style.display = "block";
    dots[slideIndex[id] - 1].className += " active";
  }

  // Automatic slideshow
  setInterval(() => {
    plusSlides(0, 1);
  }, 150000); // Change image every 5 seconds
</script>

</body>
</html>
